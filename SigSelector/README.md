
# SigSelector

SigSelector is a framework designed for efficient multi-document summarization. It usees the SigExt algorithm to evaluate and select the most relevant source documents from a cluster, significantly reducing the dimension of the prompt required for LLMs.

## Features

- **Document Selection**: Implements Top-k selection strategies based on extractive importance scores.
- **Benchmarking**: Pipeline to compare Full Text vs. Selected Text strategies.
- **Two LLM models**: Compatible with the API models GPT-3.5 (OpenAI) and Llama 3.1 (Groq).
- **Metrics**: Results are based on ROUGE and BERTScore.

## Repository Structure

- `src/`: Core modules for selecting documents and benchmarking.
- `prepare_sigext.py`: Setup script to clone, train and run inference with SigExt.
- `main.py`: Main entry point for executing experiments.

## Usage

### 1. Installation
Install the required dependencies:
```bash
pip install -r requirements.txt
```

### 2. Prepare Data & Extractor
This script clones the original SigExt repo, trains the extractor on Multi-News, and runs inference to generate importance scores.
```bash
python prepare_sigext.py
```

### 3. Execution
Run the summarization benchmark. Ensure you have your API keys set.

**Basic Run (All strategies, limited to 200 samples):**
```bash
export OPENAI_API_KEY="sk-..."
export GROQ_API_KEY="gsk_..."
python main.py
```

**Advanced Configuration:**
You can customize the number of documents (k) and the models used.

```bash
# Run only Top-1 and Top-2 selection using Llama 3.1
python main.py --k 1 2 --models llama
```

## Output

The script generates two primary output files:
1.  `sigselector_generations.csv`: Contains the raw summaries generated by the models for each strategy.
2.  `final_benchmark_metrics.csv`: A summary table reporting ROUGE-1, ROUGE-2, ROUGE-L, and BERTScore for each configuration.

## Credits
This project utilizes the **SigExt** model. Please refer to the [original repository](https://github.com/amazon-science/SigExt) for details.
