
# SigSelector

SigSelector is a framework designed for efficient multi-document summarization. It usees the **SigExt** algorithm to evaluate and select the most relevant source documents from a cluster, significantly reducing the dimension of the prompt required for Large Language Models (LLMs).

## Features

- **Document Selection**: Implements Top-k selection strategies based on extractive importance scores.
- **Models**: **GPT-3.5** (OpenAI) and **Llama 3.1** (Groq) are used for testing.
- **Benchmarking**: Pipeline to compare Full Text vs. Selected Text strategies.
- **Metrics**: Results are based on **ROUGE** and **BERTScore**.

## Repository Structure

- `src/selector.py`: Core logic for ranking and selecting documents.
- `src/benchmark.py`: Client wrapper for LLM API interactions.
- `prepare_sigext.py`: Setup script to clone, train and run inference with SigExt.
- `main.py`: Main entry point for executing experiments.

## Usage

### 1. Installation
Install the required dependencies:
```bash
pip install -r requirements.txt
```

### 2. Data Preparation
Initialize the environment and generate SigExt keyphrase data:
```bash
python prepare_sigext.py
```

### 3. Execution
Run the summarization benchmark. Ensure you have your API keys set.

**Basic Run (All strategies, limited to 200 samples):**
```bash
export OPENAI_API_KEY="sk-..."
export GROQ_API_KEY="gsk_..."
python main.py
```

**Advanced Configuration:**
You can customize the number of documents (k) and the models used.

```bash
# Run only Top-1 and Top-2 selection using Llama 3.1
python main.py --k 1 2 --models llama
```

## Output

The script generates two primary output files:
1.  `sigselector_generations.csv`: Contains the raw summaries generated by the models for each strategy.
2.  `final_benchmark_metrics.csv`: A summary table reporting ROUGE-1, ROUGE-2, ROUGE-L, and BERTScore for each configuration.
